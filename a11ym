#!/usr/bin/env node

var Crawler = require('simplecrawler');
var program = require('commander');
var process = require('process');
var URL     = require('url');
var pa11y   = require('pa11y');
var async   = require('async');

program
    .usage('[options] <url>')
    .option(
        '-d, --depth <depth>',
        'Maximum depth (hops)',
        3
    )
    .parse(process.argv);

program.url = program.args[0];

if (!program.url) {
    program.help();
    process.exit(1);
}

var runTest = new function () {
    options = {
        log: require('./reporter/html')
    };
    var test = pa11y(options);

    return function (url, callbacks) {
        try {
            test.run(
                url,
                function (error, results) {
                    if (error) {
                        options.log.error(error.stack);

                        return callbacks.error(error);
                    }

                    options.log.results(results, url);
                    callbacks.complete(results);
                }
            );
        } catch (error) {
            options.log.error(error.stack);
            process.exit(1);
        }
    };
};

var hasErrors = false;

var url  = URL.parse(program.url);
var urls = async.queue(
    function (task, onTaskComplete) {
        console.log("Run: " + task.url);
        runTest(
            task.url,
            {
                complete: function(results) {
                    onTaskComplete(null, results);
                },
                error: function(error) {
                    hasErrors = true;
                    onTaskComplete(error);
                }
            }
        );
    }
);
urls.drains = function () {
    if (true === hasErrors) {
        process.exit(2);
    }
};

var crawler             = new Crawler(url.hostname);
crawler.initialPath     = url.path;
crawler.initialPort     = url.port || 80;
crawler.initialProtocol = url.protocol || 'http';
crawler.interval        = 100;
crawler.maxConcurrency  = 5;
crawler.maxDepth        = +program.depth;

var i = 0;
crawler.on('fetchcomplete', function(queueItem) {
    if (null === queueItem.stateData.contentType.match(/^text\/html/)) {
        return;
    }

    if (++i > 2) {
        crawler.stop();
    }

    console.log("Received: " + queueItem.url);
    urls.push({url: queueItem.url});
});

crawler.start();
