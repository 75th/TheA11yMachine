#!/usr/bin/env node

'use strict';

var Crawler  = require('simplecrawler');
var URL      = require('url');
var async    = require('async');
var chalk    = require('chalk');
var pa11y    = require('pa11y');
var process  = require('process');
var program  = require('commander');
var readline = require('readline');

program
    .usage('[options] url â€¦')
    .option(
        '-c, --filter-by-codes <codes>',
        'Filter results by comma-separated WCAG codes (e.g. `H25,H91,G18`).'
    )
    .option(
        '-C, --exclude-by-codes <codes>',
        'Exclude results by comma-separated WCAG codes (e.g. `H25,H91,G18`).'
    )
    .option(
        '-l, --level <level>',
        'Level of message to fail on (exit code 2): `error` (default), `warning`, `notice`.',
        'error'
    )
    .option(
        '-d, --maximum-depth <depth>',
        'Explore up to a maximum depth (hops).',
        3
    )
    .option(
        '-m, --maximum-urls <maximum_urls>',
        'Maximum number of URLs to compute.',
        128
    )
    .option(
        '-o, --output <output_directory>',
        'Output directory.',
        __dirname + '/a11ym_output'
    )
    .option(
        '-r, --report <report>',
        'Report format: `cli`, `csv`, `html` (default), `json` or `markdown`.',
        'html'
    )
    .option(
        '-s, --standard <standard>',
        'Standard to use: `Section508`, `WCAG2A`, `WCAG2AA` (default), ` WCAG2AAA` or your own (see `--sniffers`).',
        'WCAG2AA'
    )
    .option(
        '-S, --sniffers <sniffers>',
        'Path to the sniffers file, e.g. `resource/sniffers.js` (default).',
        __dirname + '/resource/sniffers.js'
    )
    .option(
        '-u, --filter-by-urls <urls>',
        'Filter URL to test by using a regular expression without delimiters (e.g. \'news|contact\').'
    )
    .option(
        '-U, --exclude-by-urls <urls>',
        'Exclude URL to test by using a regular expression without delimiters (e.g. \'news|contact\').'
    )
    .option(
        '-w, --workers <workers>',
        'Number of workers, i.e. number of URLs computed in parallel.',
        4
    )
    .option(
        '--http-tls-disable',
        'Disable TLS/SSL when crawling or downloading pages.'
    )
    .parse(process.argv);

if (!program.args[0]) {
    program.help();
    process.exit(1);
}

var runTest = new function () {
    var reporter = null;

    if (-1 !== ['cli', 'csv', 'json'].indexOf(program.report)) {
        reporter = require('./node_modules/pa11y/reporter/' + program.report);
    } else {
        reporter = require('./reporter/html');
        reporter.config({
            outputDirectory: program.output
        });
    }

    var options = {
        log      : reporter,
        standard : program.standard,
        standards: [program.standard],
        htmlcs   : program.sniffers
    };

    var filterByCodes  = null;
    var excludeByCodes = null;

    if (undefined !== program.filterByCodes) {
        filterByCodes = new RegExp('\\b(' + program.filterByCodes.replace(/[\s,]/g, '|') + ')\\b', 'i');
    }

    if (undefined !== program.excludeByCodes) {
        excludeByCodes = new RegExp('\\b(' + program.excludeByCodes.replace(/[\s,]/g, '|') + ')\\b', 'i');
    }

    var test = pa11y(options);

    return function (url, callbacks) {
        try {
            test.run(
                url,
                function (error, results) {
                    if (error) {
                        options.log.error(error.stack);

                        return callbacks.error(error);
                    }

                    if (null !== filterByCodes) {
                        results = results.filter(
                            function (result) {
                                return filterByCodes.test(result.code);
                            }
                        );
                    }

                    if (null !== excludeByCodes) {
                        results = results.filter(
                            function (result) {
                                return !excludeByCodes.test(result.code);
                            }
                        );
                    }

                    options.log.results(results, url);

                    if (true === reportShouldFail(program.level, results)) {
                        return callbacks.error(error);
                    }

                    callbacks.complete(results);
                }
            );
        } catch (error) {
            options.log.error(error.stack);
            process.exit(1);
        }
    };
};

var reportShouldFail = function (level, results) {
    if (level === 'notice') {
        return results.length > 0;
    }

    if (level === 'warning') {
        return results.filter(isErrorOrWarning).length > 0;
    }

    return results.filter(isError).length > 0;
};

var isError = function (result) {
    return 'error' === result.type;
};

var isErrorOrWarning = function (result) {
    return 'error' === result.type || 'warning' === result.type;
};

var parseURL = function (url) {
    var parsed = URL.parse(url);

    if (!parsed.protocol) {
        parsed.protocol = 'http';
    } else {
        parsed.protocol = parsed.protocol.replace(':', '');
    }

    if (!parsed.port) {
        parsed.port = 'http' === parsed.protocol ? 80 : 443;
    }

    return parsed;
};

var maximumUrls = +program.maximumUrls;
var urlQueues = {};
var hasErrors   = false;
var isStopped   = false;
var testQueue   = async.queue(
    function (task, onTaskComplete) {
        if (--maximumUrls < 0) {
            if (false === isStopped) {
                isStopped = true;
                console.log(chalk.white.bgRed('Test queue is stopping, maximum URLs reached.'));
                testQueue.kill();

                Object
                    .keys(urlQueues)
                    .forEach(
                        function (key) {
                            urlQueues[key].kill();
                        }
                    );
            }

            onTaskComplete();

            return;
        }

        console.log(
            chalk.black.bgGreen(' ' + (program.maximumUrls - maximumUrls) + '/' + program.maximumUrls + ' ') +
            ' ' +
            chalk.black.bgGreen('[' + task.queueName + '] Run: ' + task.url + '.')
        );
        runTest(
            task.url,
            {
                complete: function(results) {
                    task.onUrlComplete();
                    onTaskComplete(null, results);
                },
                error: function(error) {
                    hasErrors = true;
                    task.onUrlComplete();
                    onTaskComplete(error);
                }
            }
        );
    },
    program.workers
);
testQueue.drain = function () {
    if (true === hasErrors) {
        process.exit(2);
    }
};

var crawler            = new Crawler();
crawler.interval       = 50;
crawler.maxConcurrency = 5;
crawler.maxDepth       = +program.maximumDepth;
crawler.filterByDomain = true;
crawler.timeout        = 10 * 1000;

if (true === program.httpTlsDisable) {
    process.env.NODE_TLS_REJECT_UNAUTHORIZED = '0';
    crawler.ignoreInvalidSSL                 = true;
}

var filterByUrls  = null;
var excludeByUrls = null;

if (undefined !== program.filterByUrls) {
    filterByUrls = new RegExp(program.filterByUrls, 'i');
}

if (undefined !== program.excludeByUrls) {
    excludeByUrls = new RegExp(program.excludeByUrls, 'i');
}

crawler
    .on(
        'fetch404',
        function(queueItem) {
            process.stderr.write(queueItem.url + ' responds with a 404.\n');
        }
    )
    .on(
        'fetcherror',
        function (queueItem) {
            process.stderr.write(
                queueItem.url + ' failed to fetch ' +
                '(status code: ' + queueItem.stateData.code + ')' +
                '.\n'
            );
        }
    ).
    on(
        'fetchcomplete',
        function(queueItem) {
            var url          = queueItem.url;
            var parsedUrl    = parseURL(url);
            var urlQueueName = (parsedUrl.pathname.split('/', 2)[1] || '__root__');

            var logPrefix = '[' + urlQueueName + '] Fetched: ' + url;

            if (!queueItem.stateData.contentType ||
                null === queueItem.stateData.contentType.match(/^text\/html/)) {
                console.log(logPrefix + '; skipped, not text/html.');

                return;
            }

            if (null !== filterByUrls && false === filterByUrls.test(url)) {
                console.log(logPrefix + '; filtered.');

                return;
            }

            if (null !== excludeByUrls && true === excludeByUrls.test(url)) {
                console.log(logPrefix + '; excluded.');

                return;
            }

            if (maximumUrls < 0) {
                crawler.stop();
                console.log(logPrefix + '; ignored, maximum URLs reached.');

                return;
            }

            console.log(logPrefix + '.');

            if (undefined === urlQueues[urlQueueName]) {
                urlQueues[urlQueueName] = async.queue(
                    function (task, onTaskComplete) {
                        console.log(chalk.magenta('[' + task.queueName + '] Waiting to run: ' + task.url) + '.');
                        testQueue.push({
                            url          : task.url,
                            queueName    : task.queueName,
                            onUrlComplete: onTaskComplete
                        });
                    }
                );
            }

            console.log(chalk.yellow('[' + urlQueueName + '] Enqueue: ' + url));
            urlQueues[urlQueueName].push({
                queueName: urlQueueName,
                url      : url
            });
        }
    );

var addURLToCrawler = function (crawler) {
    var first = true;

    return function (value) {
        var url = crawler.processURL(value);

        if (!url.host) {
            process.stderr.write('URL ' + value + ' is invalid. Ignore it.\n');

            return;
        }

        if (true === first) {
            first                   = false;
            crawler.host            = url.host;
            crawler.initialProtocol = url.protocol;
            crawler.initialPort     = url.port;
            crawler.initialPath     = url.path;
        } else {
            ++maximumUrls;
        }

        crawler.queue.add(
            url.protocol,
            url.host,
            url.port,
            url.path
        );
    };
};

if (1 === program.args.length && '-' === program.args[0]) {
    crawler.maxDepth = 1;
    readline
        .createInterface(process.stdin, undefined)
        .on('line', addURLToCrawler(crawler))
        .on('close', function () { crawler.start(); });
} else {
    if (1 < program.args.length) {
        crawler.maxDepth = 1;
    }

    program.args.forEach(addURLToCrawler(crawler));
    crawler.start();
}
