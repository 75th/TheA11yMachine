#!/usr/bin/env node

'use strict';

/**
 * Copyright (c) 2016, Ivan Enderlin and Liip
 * All rights reserved.
 *
 * Redistribution and use in source and binary forms, with or without modification,
 * are permitted provided that the following conditions are met:
 *
 * 1. Redistributions of source code must retain the above copyright notice, this
 *    list of conditions and the following disclaimer.
 *
 * 2. Redistributions in binary form must reproduce the above copyright notice,
 *    this list of conditions and the following disclaimer in the documentation
 *    and/or other materials provided with the distribution.
 *
 * 3. Neither the name of the copyright holder nor the names of its contributors
 *    may be used to endorse or promote products derived from this software without
 *    specific prior written permission.
 *
 * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND
 * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
 * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
 * DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR
 * ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
 * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
 *  LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON
 * ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
 * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
 * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 */

var Crawler  = require('simplecrawler');
var URL      = require('url');
var async    = require('async');
var chalk    = require('chalk');
var pa11y    = require('pa11y');
var process  = require('process');
var program  = require('commander');
var readline = require('readline');

// Define all the options, with their description and default value.
program
    .usage('[options] url …')
    .option(
        '-c, --filter-by-codes <codes>',
        'Filter results by comma-separated WCAG codes (e.g. `H25,H91,G18`).'
    )
    .option(
        '-C, --exclude-by-codes <codes>',
        'Exclude results by comma-separated WCAG codes (e.g. `H25,H91,G18`).'
    )
    .option(
        '-l, --level <level>',
        'Level of message to fail on (exit code 2): `error` (default), `warning`, `notice`.',
        'error'
    )
    .option(
        '-d, --maximum-depth <depth>',
        'Explore up to a maximum depth (hops).',
        3
    )
    .option(
        '-m, --maximum-urls <maximum_urls>',
        'Maximum number of URLs to compute.',
        128
    )
    .option(
        '-o, --output <output_directory>',
        'Output directory.',
        __dirname + '/a11ym_output'
    )
    .option(
        '-r, --report <report>',
        'Report format: `cli`, `csv`, `html` (default), `json` or `markdown`.',
        'html'
    )
    .option(
        '-s, --standard <standard>',
        'Standard to use: `Section508`, `WCAG2A`, `WCAG2AA` (default), ` WCAG2AAA` or your own (see `--sniffers`).',
        'WCAG2AA'
    )
    .option(
        '-S, --sniffers <sniffers>',
        'Path to the sniffers file, e.g. `resource/sniffers.js` (default).',
        __dirname + '/resource/sniffers.js'
    )
    .option(
        '-u, --filter-by-urls <urls>',
        'Filter URL to test by using a regular expression without delimiters (e.g. \'news|contact\').'
    )
    .option(
        '-U, --exclude-by-urls <urls>',
        'Exclude URL to test by using a regular expression without delimiters (e.g. \'news|contact\').'
    )
    .option(
        '-w, --workers <workers>',
        'Number of workers, i.e. number of URLs computed in parallel.',
        4
    )
    .option(
        '--http-tls-disable',
        'Disable TLS/SSL when crawling or downloading pages.'
    )
    .parse(process.argv);

// No URL to compute? Then exit.
if (!program.args[0]) {
    program.help();
    process.exit(1);
}

// Running a test includes using a PhantomJS instance to open a URL, inject
// sniffers, execute them, read the raw reports and transform them.
var runTest = new function () {
    var reporter = null;

    if (-1 !== ['cli', 'csv', 'json'].indexOf(program.report)) {
        reporter = require('./node_modules/pa11y/reporter/' + program.report);
    } else {
        reporter = require('./reporter/html');
        reporter.config({
            outputDirectory: program.output
        });
    }

    var options = {
        log      : reporter,
        standard : program.standard,
        standards: [program.standard],
        htmlcs   : program.sniffers
    };

    var filterByCodes  = null;
    var excludeByCodes = null;

    if (undefined !== program.filterByCodes) {
        filterByCodes = new RegExp('\\b(' + program.filterByCodes.replace(/[\s,]/g, '|') + ')\\b', 'i');
    }

    if (undefined !== program.excludeByCodes) {
        excludeByCodes = new RegExp('\\b(' + program.excludeByCodes.replace(/[\s,]/g, '|') + ')\\b', 'i');
    }

    var test = pa11y(options);

    return function (url, callbacks) {
        try {
            test.run(
                url,
                function (error, results) {
                    if (error) {
                        options.log.error(error.stack);

                        return callbacks.error(error);
                    }

                    if (null !== filterByCodes) {
                        results = results.filter(
                            function (result) {
                                return filterByCodes.test(result.code);
                            }
                        );
                    }

                    if (null !== excludeByCodes) {
                        results = results.filter(
                            function (result) {
                                return !excludeByCodes.test(result.code);
                            }
                        );
                    }

                    options.log.results(results, url);

                    if (true === reportShouldFail(program.level, results)) {
                        return callbacks.error(error);
                    }

                    callbacks.complete(results);
                }
            );
        } catch (error) {
            options.log.error(error.stack);
            process.exit(1);
        }
    };
};

// Define when the process should fail or not.
var reportShouldFail = function (level, results) {
    if (level === 'notice') {
        return results.length > 0;
    }

    if (level === 'warning') {
        return results.filter(isErrorOrWarning).length > 0;
    }

    return results.filter(isError).length > 0;
};

// Is the result an error?
var isError = function (result) {
    return 'error' === result.type;
};

// Is the result an error or a warning?
var isErrorOrWarning = function (result) {
    return 'error' === result.type || 'warning' === result.type;
};

// Parse a URL, canonize them and set default values.
var parseURL = function (url) {
    var parsed = URL.parse(url);

    if (!parsed.protocol) {
        parsed.protocol = 'http';
    } else {
        // Remove the trailing colon in the protocol.
        parsed.protocol = parsed.protocol.replace(':', '');
    }

    if (!parsed.port) {
        parsed.port = 'http' === parsed.protocol ? 80 : 443;
    }

    return parsed;
};

// Maximum number of URL to compute.
var maximumUrls = +program.maximumUrls;

// Each URL is dispatched in a specific bucket. So far, a bucket is defined as
// the first part of the pathname. For instance let's consider the `/a/b/c` URL;
// its bucket is `a`. Each bucket is actually a queue. They are all computed in
// parallel. Each queue waits for the outgoing URL to be totally consumed by the
// test queue (see bellow) before sending another one.
var urlQueues = {};

// When an error occurs, update this flag. It will change the exit code of the
// process.
var hasErrors = false;

// When the test queue has been stopped once, update this flag. It avoids to do
// the “stop computation” more than once.
var isStopped = false;

// Queue of URL waiting to be tested (see the `runTest` function). This queue
// has a user-defined concurrency level.
// When a test is executed, it calls the “complete” callback on the task.
// When the maximum number of URL is reached, then we kill this queue, in
// addition to kill all URL queues.
var testQueue = async.queue(
    function (task, onTaskComplete) {
        if (--maximumUrls < 0) {
            if (false === isStopped) {
                isStopped = true;
                console.log(chalk.white.bgRed('Test queue is stopping, maximum URLs reached.'));
                testQueue.kill();

                Object
                    .keys(urlQueues)
                    .forEach(
                        function (key) {
                            urlQueues[key].kill();
                        }
                    );
            }

            onTaskComplete();

            return;
        }

        console.log(
            chalk.black.bgGreen(' ' + (program.maximumUrls - maximumUrls) + '/' + program.maximumUrls + ' ') +
            ' ' +
            chalk.black.bgGreen('[' + task.queueName + '] Run: ' + task.url + '.')
        );
        runTest(
            task.url,
            {
                // When test succeed.
                complete: function(results) {
                    task.onUrlComplete();
                    onTaskComplete(null, results);
                },
                // When test failed.
                error: function(error) {
                    hasErrors = true;
                    task.onUrlComplete();
                    onTaskComplete(error);
                }
            }
        );
    },
    program.workers
);
testQueue.drain = function () {
    if (true === hasErrors) {
        process.exit(2);
    }
};

// Set the crawler.
var crawler            = new Crawler();
crawler.interval       = 50;
crawler.maxConcurrency = 5;
crawler.maxDepth       = +program.maximumDepth;
crawler.filterByDomain = true;
crawler.timeout        = 10 * 1000;
crawler.userAgent      = 'liip/a11ym';

if (true === program.httpTlsDisable) {
    process.env.NODE_TLS_REJECT_UNAUTHORIZED = '0';
    crawler.ignoreInvalidSSL                 = true;
}

var filterByUrls  = null;
var excludeByUrls = null;

if (undefined !== program.filterByUrls) {
    filterByUrls = new RegExp(program.filterByUrls, 'i');
}

if (undefined !== program.excludeByUrls) {
    excludeByUrls = new RegExp(program.excludeByUrls, 'i');
}

crawler
    .on(
        'fetch404',
        function(queueItem) {
            process.stderr.write(queueItem.url + ' responds with a 404.\n');
        }
    )
    .on(
        'fetcherror',
        function (queueItem) {
            process.stderr.write(
                queueItem.url + ' failed to fetch ' +
                '(status code: ' + queueItem.stateData.code + ')' +
                '.\n'
            );
        }
    ).
    on(
        'fetchcomplete',
        function(queueItem) {
            var url       = queueItem.url;
            var parsedUrl = parseURL(url);

            // Compute the URL bucket name.
            var urlQueueName = (parsedUrl.pathname.split('/', 2)[1] || '__root__');

            var logPrefix = '[' + urlQueueName + '] Fetched: ' + url;

            // Filter by content-type.
            if (!queueItem.stateData.contentType ||
                null === queueItem.stateData.contentType.match(/^text\/html/)) {
                console.log(logPrefix + '; skipped, not text/html.');

                return;
            }

            // Filter by URL.
            if (null !== filterByUrls && false === filterByUrls.test(url)) {
                console.log(logPrefix + '; filtered.');

                return;
            }

            // Exclude by URL.
            if (null !== excludeByUrls && true === excludeByUrls.test(url)) {
                console.log(logPrefix + '; excluded.');

                return;
            }

            // The maximum number of URL is reached. Stop the crawler, but its
            // queue is not emptied.
            if (maximumUrls < 0) {
                crawler.stop();
                console.log(logPrefix + '; ignored, maximum URLs reached.');

                return;
            }

            console.log(logPrefix + '.');

            // Create the URL “bucket”.
            if (undefined === urlQueues[urlQueueName]) {
                urlQueues[urlQueueName] = async.queue(
                    function (task, onTaskComplete) {
                        console.log(chalk.magenta('[' + task.queueName + '] Waiting to run: ' + task.url) + '.');
                        testQueue.push({
                            url          : task.url,
                            queueName    : task.queueName,
                            onUrlComplete: onTaskComplete
                        });
                    }
                );
            }

            console.log(chalk.yellow('[' + urlQueueName + '] Enqueue: ' + url));

            // Feed the URL “bucket” with a new URL.
            urlQueues[urlQueueName].push({
                queueName: urlQueueName,
                url      : url
            });
        }
    );

// Helper to add a URL to the crawler queue.
var addURLToCrawler = function (crawler) {
    var first = true;

    return function (value) {
        var url = crawler.processURL(value);

        if (!url.host) {
            process.stderr.write('URL ' + value + ' is invalid. Ignore it.\n');

            return;
        }

        if (true === first) {
            first                   = false;
            crawler.host            = url.host;
            crawler.initialProtocol = url.protocol;
            crawler.initialPort     = url.port;
            crawler.initialPath     = url.path;
        } else {
            ++maximumUrls;
        }

        crawler.queue.add(
            url.protocol,
            url.host,
            url.port,
            url.path
        );
    };
};

if (1 === program.args.length && '-' === program.args[0]) {
    // Read a list of URL from STDIN.
    crawler.maxDepth = 1;
    readline
        .createInterface(process.stdin, undefined)
        .on('line', addURLToCrawler(crawler))
        .on('close', function () { crawler.start(); });
} else {
    // Read a list of URL from the command-line.
    if (1 < program.args.length) {
        crawler.maxDepth = 1;
    }

    program.args.forEach(addURLToCrawler(crawler));
    crawler.start();
}
